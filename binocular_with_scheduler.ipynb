{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aac0ba8",
   "metadata": {},
   "source": [
    "# Binocular\n",
    "\n",
    "We explore a different approach of training per patient by loading both left and right image and train efficient net b0 \n",
    "\n",
    "We also treat ODIR as multi-label problem instead of multi-class as originally it is officially a multi-label problem\n",
    "from https://odir2019.grand-challenge.org/dataset/\n",
    "> Note: one patient may contains one or multiple labels. \n",
    "\n",
    "We also want to explore binocular or siamese approach to train our model on both left and right fundus image pair. This has been researched in https://arxiv.org/html/2504.18046v3 DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular: Fundus Image Classification Guohao Huo, Zibo Lin, Zitong Wang, Ruiting Dai, Hao Tang paper to work well for fundus disease classification \n",
    "\n",
    "There are 3 advantages of use both eyes images instead of one eye image :\n",
    "- Symmetry: Diseases like Diabetes aren't \"accidents\" in one eye; they are systemic. If the AI sees it in both, it's a \"confirmed\" diagnosis.\n",
    "\n",
    "- Comparison: The left eye acts as a \"control\" for the right eye. AI can spot a tiny change by noticing how much it differs from the other eye.\n",
    "\n",
    "- Noise Reduction: Just like your two eyes help you see depth, two images help the AI ignore \"camera blur\" or \"dust\" on one lens that might look like a disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7e6c1",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7fd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q kagglehub torch torchvision scikit-learn pandas opencv-python tqdm wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a9960",
   "metadata": {},
   "source": [
    "Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c344cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix, accuracy_score\n",
    "import kagglehub\n",
    "from tqdm import tqdm # tqdm for progress bars\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6af03",
   "metadata": {},
   "source": [
    "Download DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae4a811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/ray/.cache/kagglehub/datasets/andrewmvd/ocular-disease-recognition-odir5k/versions/2\n"
     ]
    }
   ],
   "source": [
    "# 1. Download Dataset (Official ODIR-5K)\n",
    "path = kagglehub.dataset_download(\"andrewmvd/ocular-disease-recognition-odir5k\")\n",
    "print(\"Dataset path:\", path)\n",
    "IMG_DIR = os.path.join(path, \"ODIR-5K/ODIR-5K/Training Images\")\n",
    "CSV_PATH = os.path.join(path, \"full_df.csv\")\n",
    "TRAIN_CSV_PATH = \"train.csv\"\n",
    "VAL_CSV_PATH =  \"val.csv\"\n",
    "TEST_CSV_PATH =  \"test.csv\"\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "ACCUMULATION_STEPS = 8\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5 # Early stopping patience in epochs, stop if no improvement in F1 score for this many epochs\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 8\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "val_df = pd.read_csv(VAL_CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "FAST_IMG_DIR = f\"tmp/processed_{IMG_SIZE}_images\"\n",
    "os.makedirs(FAST_IMG_DIR, exist_ok=True)\n",
    "RUN_NAME = f\"efficient-b4_bbgsa_{IMG_SIZE}\" # Unique name for this run, used for saving models and logging\n",
    "SAVED_MODELS_DIR = \"saved_models\"\n",
    "os.makedirs(SAVED_MODELS_DIR, exist_ok=True)\n",
    "SAVED_MODEL_PATH = os.path.join(SAVED_MODELS_DIR, f\"{RUN_NAME}_best.pth\")\n",
    "CHECKPOINT_PATH = os.path.join(SAVED_MODELS_DIR, f\"{RUN_NAME}_checkpoint.pth\")\n",
    "CLASS_NAMES = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Other']\n",
    "CLASS_CODES = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7f5d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraymond-samalo\u001b[0m (\u001b[33msamalo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ray/Projects/odir-200226/wandb/run-20260221_235159-rjmsj0ec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samalo/odir-2019/runs/rjmsj0ec' target=\"_blank\">efficient-b4_bbgsa_512</a></strong> to <a href='https://wandb.ai/samalo/odir-2019' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samalo/odir-2019' target=\"_blank\">https://wandb.ai/samalo/odir-2019</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samalo/odir-2019/runs/rjmsj0ec' target=\"_blank\">https://wandb.ai/samalo/odir-2019/runs/rjmsj0ec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/samalo/odir-2019/runs/rjmsj0ec?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x758a01d5bef0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"odir-2019\", name=RUN_NAME, config={\n",
    "    \"img_size\": IMG_SIZE, \"lr\": 1e-4, \n",
    "    \"batch_size\": BATCH_SIZE, \"accumulation_steps\": ACCUMULATION_STEPS, \n",
    "    \"epochs\": EPOCHS, \"patience\": PATIENCE})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e383c0",
   "metadata": {},
   "source": [
    "## Ben Graham's Preprocessing\n",
    "\n",
    "This function implements the Ben Graham Preprocessing \n",
    "ref : \n",
    "- https://scholar.google.com/citations?view_op=view_citation&hl=en&user=jQkkhlkAAAAJ&citation_for_view=jQkkhlkAAAAJ:sNmaIFBj_lkC\n",
    "- https://scholar.google.com/citations?user=jQkkhlkAAAAJ&hl=en\n",
    "\n",
    "\n",
    "From https://medium.com/@astronomer.abdurrehman/enhancing-image-quality-for-machine-learning-ben-grahams-preprocessing-e795ad982abe\n",
    "the method described as followed\n",
    "<blockquote>\n",
    "\n",
    "The cv2.GauissanBlur takes an image, (0, 0) tuple automatically chooses a gaussian filter size based on sigmaX value which specifies the intensity of blur. Goal of using gaussian blur here is to reduce the noise and smooth out the fine details.\n",
    "\n",
    "The addWeighted function blends two images together using specified weights, the -4 here is the beta value which subtracts the blurred image from the original image and 128 is the gamma value that adjusts the brightness so that the image does not become too dark after subtraction.\n",
    "\n",
    "\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e956c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ben_graham_prep(img, sigmaX=10):\n",
    "    \"\"\"Enhances vessels and normalizes lighting.\"\"\"\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Circular Crop: Find non-black pixels and crop\n",
    "    mask = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) > 10\n",
    "    if np.any(mask):\n",
    "        coords = np.argwhere(mask)\n",
    "        y0, x0 = coords.min(axis=0)\n",
    "        y1, x1 = coords.max(axis=0)\n",
    "        img = img[y0:y1, x0:x1]\n",
    "    \n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    blurred = cv2.GaussianBlur(img, (0, 0), sigmaX)\n",
    "    enhanced = cv2.addWeighted(img, 4, blurred, -4, 128)\n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d3b4b",
   "metadata": {},
   "source": [
    "On the fly image prep caused the training slowdown given the image need to be preprocessed repeatedly each time it is loaded. We speed up the process by performing preprocessing offline once and cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3bd455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_offline_prep(df, raw_dir, img_prep_func, save_dir):\n",
    "    print(\"ðŸš€ Starting Offline Pre-processing (Ben Graham)...\")\n",
    "    all_images = pd.concat([df['Left-Fundus'], df['Right-Fundus']]).unique()\n",
    "    for img_name in tqdm(all_images):\n",
    "        save_path = os.path.join(save_dir, img_name)\n",
    "        if not os.path.exists(save_path):\n",
    "            img = cv2.imread(os.path.join(raw_dir, img_name))\n",
    "            # Ben Graham Logic\n",
    "            enhanced = img_prep_func(img)\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(enhanced, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd0e0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Offline Pre-processing (Ben Graham)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6716/6716 [00:00<00:00, 412562.55it/s]\n"
     ]
    }
   ],
   "source": [
    "run_offline_prep(df, IMG_DIR, ben_graham_prep, FAST_IMG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae33d8a",
   "metadata": {},
   "source": [
    "## Data Loader \n",
    "\n",
    "Previously we load the data and then performed preprocessing on the fly\n",
    "Given we did the preprocessing offline, we can now simply load the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b82fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastODIRDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        # Target classes: Normal, Diabetes, Glaucoma, Cataract, AMD, Hypertension, Myopia, Other\n",
    "        self.labels = df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        l_img_path = os.path.join(self.img_dir, row['Left-Fundus'])\n",
    "        r_img_path = os.path.join(self.img_dir, row['Right-Fundus'])\n",
    "        \n",
    "        # Load and Preprocess\n",
    "        l_img = cv2.cvtColor(cv2.imread(l_img_path), cv2.COLOR_BGR2RGB)\n",
    "        r_img = cv2.cvtColor(cv2.imread(r_img_path), cv2.COLOR_BGR2RGB )\n",
    "        \n",
    "        if self.transform:\n",
    "            l_img = self.transform(l_img)\n",
    "            r_img = self.transform(r_img)\n",
    "            \n",
    "        return l_img, r_img, torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f13a857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDualNet(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        # Using B0 for efficiency, upgrade to B4 for better accuracy\n",
    "        self.backbone = models.efficientnet_b4(weights='DEFAULT') # use pretrained weights for better feature extraction\n",
    "        # ref https://docs.pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights\n",
    "        self.feature_dim = self.backbone.classifier[1].in_features # Get feature dimension before classifier\n",
    "        self.backbone.classifier = nn.Identity() # Remove top layer\n",
    "\n",
    "        self.classifier = nn.Sequential( # replace classifier with a custom head that combines features from both eyes\n",
    "            nn.Linear(self.feature_dim * 2, IMG_SIZE), # Combine features from both eyes\n",
    "            nn.ReLU(), # Non-linearity for better learning relu f(x) = max(0, x)\n",
    "            nn.Dropout(0.3), # Regularization to prevent overfitting\n",
    "            nn.Linear(IMG_SIZE, num_classes) # Final output layer for multi-label classification\n",
    "        )\n",
    "\n",
    "    def forward(self, left, right):\n",
    "        l_feat = self.backbone(left) # Extract features from left eye\n",
    "        r_feat = self.backbone(right) # Extract features from right eye\n",
    "        combined = torch.cat((l_feat, r_feat), dim=1) # Combine features from both eyes\n",
    "        return self.classifier(combined) # Pass through classifier to get final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b359c",
   "metadata": {},
   "source": [
    "## Thresholds\n",
    "\n",
    "Instead of using 1 thresholds 0.5 for all labels, we find the best or optimise threshold for each label individually to maximise our F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b43d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thresholds(y_true, y_probs):\n",
    "    thresholds = np.linspace(0.1, 0.9, 81) # Test thresholds from 0.1 to 0.9 with fine granularity\n",
    "    best_ts = np.zeros(NUM_CLASSES) \n",
    "    for i in range(NUM_CLASSES):\n",
    "        best_f1 = 0 # Initialize best F1 score for this class\n",
    "        for t in thresholds: # Test each threshold and calculate F1 score \n",
    "            score = f1_score(y_true[:, i], (y_probs[:, i] > t).astype(int), zero_division=0) # zero_division=0 to handle cases where there are no positive predictions\n",
    "            if score > best_f1: \n",
    "                best_f1 = score # Update best F1 score for this class\n",
    "                best_ts[i] = t # Update best threshold for this class\n",
    "    return best_ts # Return array of best thresholds for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4e314",
   "metadata": {},
   "source": [
    "Data Loader with ImageNet Transformation\n",
    "\n",
    "\n",
    "\"All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\"\n",
    "Ref:\n",
    "- https://docs.pytorch.org/vision/0.9/models.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0970a94",
   "metadata": {},
   "source": [
    "model definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd5a3353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /home/ray/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74.5M/74.5M [00:07<00:00, 9.80MB/s]\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nSearch for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     29\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(FastODIRDataset(val_df, FAST_IMG_DIR, transform), \n\u001b[1;32m     30\u001b[0m                         batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     31\u001b[0m                         num_workers\u001b[38;5;241m=\u001b[39mNUM_WORKERS, \n\u001b[1;32m     32\u001b[0m                         pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(FastODIRDataset(test_df, FAST_IMG_DIR, transform), \n\u001b[1;32m     34\u001b[0m                          batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     35\u001b[0m                          num_workers\u001b[38;5;241m=\u001b[39mNUM_WORKERS, \n\u001b[1;32m     36\u001b[0m                          pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mODIRDualNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m     39\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1371\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 930\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 930\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 930 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 930\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    934\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:957\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 957\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1357\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1352\u001b[0m             device,\n\u001b[1;32m   1353\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1354\u001b[0m             non_blocking,\n\u001b[1;32m   1355\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1356\u001b[0m         )\n\u001b[0;32m-> 1357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nSearch for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch import autocast\n",
    "from torch.amp.grad_scaler import  GradScaler \n",
    "\n",
    "# val and test transforms (no augmentation, just normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# Data augmentation for training set \n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # Note: Horizontal/Vertical flips may not be appropriate for medical images as they can change the anatomical orientation. Use with caution.\n",
    "        # hence commented out for now, can be enabled if you want to experiment with it\n",
    "        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomVerticalFlip(p=0.5),\n",
    "        # Rotation and Color Jitter can help the model generalize better by simulating real-world variations in the images\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_loader = DataLoader(FastODIRDataset(train_df, FAST_IMG_DIR, train_transform), \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=True)\n",
    "val_loader = DataLoader(FastODIRDataset(val_df, FAST_IMG_DIR, transform), \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=NUM_WORKERS, \n",
    "                        pin_memory=True)\n",
    "test_loader = DataLoader(FastODIRDataset(test_df, FAST_IMG_DIR, transform), \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=NUM_WORKERS, \n",
    "                         pin_memory=True)\n",
    "model = ODIRDualNet().to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "scaler = GradScaler(DEVICE)\n",
    "# âš¡ ACCELERATION: Compile the model (Requires PyTorch 2.0+)\n",
    "# This can provide a 10-20% speedup in training time\n",
    "if hasattr(torch, 'compile'):\n",
    "    model = torch.compile(model)\n",
    "    print(\"âœ… Model Compiled for speed.\")\n",
    "\n",
    "start_epoch, best_f1, counter = 0, 0, 0\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    best_ts = checkpoint['thresholds']\n",
    "    counter = checkpoint.get('counter', 0)\n",
    "    start_epoch, best_f1, counter = checkpoint['epoch'] + 1, checkpoint['best_f1'], checkpoint.get('counter', 0)\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        tr_preds, tr_true = [], []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (l, r, y) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            l, r, y = l.to(DEVICE), r.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            with autocast(device_type=DEVICE):\n",
    "                preds = model(l, r) # logits output from the model\n",
    "                loss = criterion(preds, y) / ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            tr_preds.append(torch.sigmoid(preds).detach().cpu().numpy())\n",
    "            tr_true.append(y.cpu().numpy())\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_preds, val_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for l, r, y in val_loader:\n",
    "                out = torch.sigmoid(model(l.to(DEVICE), r.to(DEVICE)))\n",
    "                val_preds.append(out.cpu().numpy())\n",
    "                val_true.append(y.numpy())\n",
    "        \n",
    "        val_probs = np.vstack(val_preds)\n",
    "        val_true = np.vstack(val_true)\n",
    "        best_ts = find_best_thresholds(val_true, val_probs)\n",
    "        tr_probs, tr_true = np.vstack(tr_preds), np.vstack(tr_true)\n",
    "        # Calculate Macro F1 with optimized thresholds\n",
    "        val_preds_binary = (val_probs > best_ts).astype(int)\n",
    "        tr_preds_binary = (tr_probs > best_ts).astype(int)\n",
    "        val_f1 = f1_score(val_true, val_preds_binary, average='macro')\n",
    "        train_f1 = f1_score(tr_true, tr_preds_binary, average='macro')\n",
    "        train_acc = accuracy_score(tr_true, tr_preds_binary)\n",
    "        val_acc = accuracy_score(val_true, val_preds_binary)\n",
    "        # NEW: Step the scheduler based on Validation F1\n",
    "        scheduler.step(val_f1)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Log per-class F1 for visibility\n",
    "        per_class_f1 = f1_score(val_true, (val_probs > best_ts).astype(int), average=None)\n",
    "        metrics_dict = {f\"val_f1_{name}\": f for name, f in zip(CLASS_NAMES, per_class_f1)}\n",
    "        metrics_dict.update({\"epoch\": epoch+1, \"val_f1\": val_f1, \"train_f1\": train_f1, \"lr\": current_lr, \"train_loss\": train_loss / len(train_loader),\n",
    "                             \"train_acc\": train_acc, \"val_acc\": val_acc})\n",
    "        wandb.log(metrics_dict)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {train_loss/len(train_loader):.4f} | train_f1: {train_f1:.4f} | val_f1: {val_f1:.4f}\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            counter = 0 # reset counter on improvement\n",
    "            torch.save({'model': model.state_dict(), 'thresholds': best_ts}, SAVED_MODEL_PATH)\n",
    "            print(f\"ðŸš€ New Best Model Saved! F1: {val_f1:.4f}\")\n",
    "        # Save checkpoint every 5 epochs or if no improvement for 3 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'best_f1': best_f1,\n",
    "                'counter': counter,\n",
    "                'thresholds': best_ts\n",
    "            }, CHECKPOINT_PATH)\n",
    "            print(f\"ðŸ’¾ Checkpoint saved at epoch {epoch+1}\")\n",
    "        if  PATIENCE>0 and counter >= PATIENCE: # early stopping if no improvement for 3 epochs\n",
    "            print(\"â¹ï¸ Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00ff47",
   "metadata": {},
   "source": [
    "## Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(SAVED_MODEL_PATH, weights_only=False)\n",
    "thresholds = best_model['thresholds']\n",
    "model.load_state_dict(best_model['model'])\n",
    "print(\"Label Thresholds and Macro F1 Score of the Best Model:\")\n",
    "print(\"Labels: Normal, Diabetes, Glaucoma, Cataract, AMD, Hypertension, Myopia, Other\")\n",
    "print(\"Best Thresholds:\", thresholds)\n",
    "print(\"Best Macro F1 Score:\", best_f1)\n",
    "print(\"âœ… Training Complete. Best model and thresholds saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "t_probs, t_true = [], []\n",
    "with torch.no_grad():\n",
    "    for l, r, y in test_loader:\n",
    "        out = torch.sigmoid(model(l.to(DEVICE), r.to(DEVICE)))\n",
    "        t_probs.append(out.cpu().numpy()); t_true.append(y.numpy())\n",
    "\n",
    "t_p, t_t = np.vstack(t_probs), np.vstack(t_true)\n",
    "t_preds = (t_p > thresholds).astype(int)\n",
    "\n",
    "mcm = multilabel_confusion_matrix(t_t, t_preds)\n",
    "print(\"Multilabel Confusion Matrix:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    tn, fp, fn, tp = mcm[i].ravel()\n",
    "    print(f\"{class_name}: TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n",
    "    wandb.log({f\"CM_{class_name}\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=t_t[:, i],\n",
    "            preds=t_preds[:, i],\n",
    "            title=f\"Confusion Matrix for {class_name}\",\n",
    "            class_names=[\"Absent\", \"Present\"]\n",
    "        )})\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(t_t, t_preds, target_names=CLASS_NAMES))\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
