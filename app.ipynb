{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3975ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q  torch torchvision scikit-learn pandas opencv-python torchinfo gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a881321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "NUM_CLASSES=8\n",
    "IMG_SIZE = 512\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class ODIRDualNet(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        # Using B0 for efficiency, upgrade to B4 for better accuracy\n",
    "        self.backbone = models.efficientnet_b0(weights='DEFAULT') # use pretrained weights for better feature extraction\n",
    "        # ref https://docs.pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights\n",
    "        self.feature_dim = self.backbone.classifier[1].in_features # Get feature dimension before classifier\n",
    "        self.backbone.classifier = nn.Identity() # Remove top layer\n",
    "        self.features = self.backbone.features # Extract feature extractor part for checkpointing\n",
    "        self.classifier = nn.Sequential( # replace classifier with a custom head that combines features from both eyes\n",
    "            nn.Linear(self.feature_dim * 2, IMG_SIZE), # Combine features from both eyes\n",
    "            nn.ReLU(), # Non-linearity for better learning relu f(x) = max(0, x)\n",
    "            nn.Dropout(0.3), # Regularization to prevent overfitting\n",
    "            nn.Linear(IMG_SIZE, num_classes) # Final output layer for multi-label classification\n",
    "        )\n",
    "\n",
    "    def forward(self, left, right):\n",
    "        # manually checkpoint the feature extraction part to save memory, since EfficientNet can be quite large, especially B4\n",
    "        l_feat = checkpoint(self.features, left, use_reentrant=False)\n",
    "        r_feat = checkpoint(self.features, right, use_reentrant=False)\n",
    "        \n",
    "        # Global Average Pooling to get (Batch, Feat_Dim)\n",
    "        l_feat = torch.flatten(nn.functional.adaptive_avg_pool2d(l_feat, 1), 1)\n",
    "        r_feat = torch.flatten(nn.functional.adaptive_avg_pool2d(r_feat, 1), 1)\n",
    "        combined = torch.cat((l_feat, r_feat), dim=1) # Combine features from both eyes\n",
    "        return self.classifier(combined) # Pass through classifier to get final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfeeb65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Compiled for speed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "SAVED_MODELS_DIR = \"saved_models\"\n",
    "os.makedirs(SAVED_MODELS_DIR, exist_ok=True)\n",
    "IMAGE_PREP_NAME = \"gamma\" # Name for this image pre-processing method, used for directory naming and logging\n",
    "RUN_NAME = f\"efficient-b0_{IMAGE_PREP_NAME}\" # Unique name for this run, used for saving models and logging\n",
    "SAVED_MODEL_PATH = os.path.join(SAVED_MODELS_DIR, f\"{RUN_NAME}_best.pth\")\n",
    "best_model = torch.load(SAVED_MODEL_PATH, weights_only=False)\n",
    "thresholds = best_model['thresholds']\n",
    "CLASS_NAMES = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Other']\n",
    "\n",
    "model = ODIRDualNet().to(DEVICE)\n",
    "if hasattr(torch, 'compile'):\n",
    "    model = torch.compile(model)\n",
    "    print(\"‚úÖ Model Compiled for speed.\")\n",
    "model.load_state_dict(best_model['model'])\n",
    "thresholds = best_model['thresholds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f279ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "from preprocessing import custom_gamma\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def preprocessing(img_pil_rgb):\n",
    "    img_np = np.array(img_pil_rgb)\n",
    "    img = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "    img = custom_gamma(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4171aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_208878/2952502758.py:61: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(theme=gr.themes.Monochrome()) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images \n",
      "images \n",
      "images \n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "\n",
    "\n",
    "\n",
    "# 2. PREDICTION LOGIC\n",
    "def predict(left_img, right_img):\n",
    "    if left_img is None and right_img is None:\n",
    "        return None, None, None, \"‚ùå Please upload at least one image.\"\n",
    "    \n",
    "    info_msg = \"‚úÖ Binocular analysis complete.\"\n",
    "    print(\"images \")\n",
    "    \n",
    "    # Handle single upload\n",
    "    if left_img is None:\n",
    "        left_img = right_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        info_msg = \"‚ö†Ô∏è Using flipped Right eye for missing Left eye.\"\n",
    "    elif right_img is None:\n",
    "        right_img = left_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        info_msg = \"‚ö†Ô∏è Using flipped Left eye for missing Right eye.\"\n",
    "\n",
    "    # Process images for the model\n",
    "    # proc_l_pil = medical_prep(left_img)\n",
    "    # proc_r_pil = medical_prep(right_img)\n",
    "    proc_l_pil = preprocessing(left_img)\n",
    "    proc_r_pil = preprocessing(right_img)\n",
    "    \n",
    "    # Convert to Tensor for model\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    l_tensor = transform(proc_l_pil).unsqueeze(0).to(DEVICE)\n",
    "    r_tensor = transform(proc_r_pil).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(l_tensor, r_tensor)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "\n",
    "    results={}\n",
    "    diagnoses=[]\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        p = float(probs[i])\n",
    "        t = thresholds[i]\n",
    "        results[name] = p \n",
    "        if p>=t:\n",
    "            diagnoses.append(f\"**{name}** (Prob: {p:.2f} > Thr: {t:.2f})\")\n",
    "    if not diagnoses:\n",
    "        info_msg = \"### Summary: No diseases detected.\"\n",
    "    else:\n",
    "        info_msg = \"### üö© Detected Conditions:\\n\" + \"\\n\".join(diagnoses)\n",
    "    # Return processed PIL images for display, results, and msg\n",
    "    return proc_l_pil, proc_r_pil, results, info_msg\n",
    "\n",
    "# 3. INTERFACE DESIGN\n",
    "with gr.Blocks(theme=gr.themes.Monochrome()) as demo:\n",
    "    gr.Markdown(\"# üëÅÔ∏è ODIR Diagnostic Dashboard\")\n",
    "    gr.Markdown(''' \n",
    "                ```\n",
    "                Some images that we can try : \n",
    "                    - 112_left.jpg,112_right.jpg,normal fundus,cataract\n",
    "                    - 43_left.jpg,43_right.jpg,wet age-related macular degeneration,dry age-related macular degenerationÔºåglaucoma\n",
    "                    - 32_left.jpg,32_right.jpg,hypertensive retinopathy\n",
    "                    - 71_left.jpg,71_right.jpg,diabetic retinopathy,wet age-related macular degenerationÔºådiabetic retinopathy\n",
    "                ```                \n",
    "                ''')    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 1. Upload Raw Images\")\n",
    "            with gr.Column():\n",
    "                in_l = gr.Image(label=\"Raw Left Eye\", type=\"pil\", width=256, height=256)\n",
    "                in_r = gr.Image(label=\"Raw Right Eye\", type=\"pil\", width=256, height=256)\n",
    "            btn = gr.Button(\"Process & Diagnose\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 2. Model's View (Processed)\")\n",
    "            out_l = gr.Image(label=\"Processed Left\", interactive=False, width=256, height=256)\n",
    "            out_r = gr.Image(label=\"Processed Right\", interactive=False, width=256, height=256)\n",
    "            \n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 3. Diagnostic Results\")\n",
    "            out_label = gr.Label(num_top_classes=8)\n",
    "            status = gr.Markdown(\"Status: Waiting for input...\")\n",
    "\n",
    "    btn.click(\n",
    "        fn=predict, \n",
    "        inputs=[in_l, in_r], \n",
    "        outputs=[out_l, out_r, out_label, status]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
