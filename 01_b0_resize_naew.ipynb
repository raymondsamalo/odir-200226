{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aac0ba8",
   "metadata": {},
   "source": [
    "# B0 RESIZE BINOCULAR without AUGMENTATION extra weight\n",
    "\n",
    "We explore a different approach of training per patient by loading both left and right image and train efficient net b0 \n",
    "\n",
    "We also treat ODIR as multi-label problem instead of multi-class as originally it is officially a multi-label problem\n",
    "from https://odir2019.grand-challenge.org/dataset/\n",
    "> Note: one patient may contains one or multiple labels. \n",
    "\n",
    "We also want to explore binocular or siamese approach to train our model on both left and right fundus image pair. This has been researched in https://arxiv.org/html/2504.18046v3 DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular: Fundus Image Classification Guohao Huo, Zibo Lin, Zitong Wang, Ruiting Dai, Hao Tang paper to work well for fundus disease classification \n",
    "\n",
    "There are 3 advantages of use both eyes images instead of one eye image :\n",
    "- Symmetry: Diseases like Diabetes aren't \"accidents\" in one eye; they are systemic. If the AI sees it in both, it's a \"confirmed\" diagnosis.\n",
    "\n",
    "- Comparison: The left eye acts as a \"control\" for the right eye. AI can spot a tiny change by noticing how much it differs from the other eye.\n",
    "\n",
    "- Noise Reduction: Just like your two eyes help you see depth, two images help the AI ignore \"camera blur\" or \"dust\" on one lens that might look like a disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7e6c1",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7fd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q  torch torchvision scikit-learn pandas opencv-python tqdm wandb torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a9960",
   "metadata": {},
   "source": [
    "Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c344cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm # tqdm for progress bars\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6af03",
   "metadata": {},
   "source": [
    "Download DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae4a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PREP_NAME = \"resize\" # Name for this image pre-processing method, used for directory naming and logging\n",
    "IMG_DIR = f\"tmp/{IMAGE_PREP_NAME}_prep\" # Directory where images are stored, adjust if needed\n",
    "RUN_NAME = f\"efficient-b0_{IMAGE_PREP_NAME}_naew\" # Unique name for this run, used for saving models and logging\n",
    "\n",
    "TRAIN_CSV_PATH = \"train.csv\"\n",
    "VAL_CSV_PATH =  \"val.csv\"\n",
    "TEST_CSV_PATH =  \"test.csv\"\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "val_df = pd.read_csv(VAL_CSV_PATH)\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "ACCUMULATION_STEPS = 8\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5 # Early stopping patience in epochs, stop if no improvement in F1 score for this many epochs\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 8\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVED_MODELS_DIR = \"saved_models\"\n",
    "os.makedirs(SAVED_MODELS_DIR, exist_ok=True)\n",
    "SAVED_MODEL_PATH = os.path.join(SAVED_MODELS_DIR, f\"{RUN_NAME}_best.pth\")\n",
    "CHECKPOINT_PATH = os.path.join(SAVED_MODELS_DIR, f\"{RUN_NAME}_checkpoint.pth\")\n",
    "CLASS_NAMES = ['Normal', 'Diabetes', 'Glaucoma', 'Cataract', 'AMD', 'Hypertension', 'Myopia', 'Other']\n",
    "CLASS_CODES = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7f5d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraymond-samalo\u001b[0m (\u001b[33msamalo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ray/Projects/odir-200226/wandb/run-20260224_092735-a652j226</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samalo/odir-2019-b/runs/a652j226' target=\"_blank\">efficient-b0_resize_naew</a></strong> to <a href='https://wandb.ai/samalo/odir-2019-b' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samalo/odir-2019-b' target=\"_blank\">https://wandb.ai/samalo/odir-2019-b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samalo/odir-2019-b/runs/a652j226' target=\"_blank\">https://wandb.ai/samalo/odir-2019-b/runs/a652j226</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/samalo/odir-2019-b/runs/a652j226?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x72287497aab0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"odir-2019-b\", name=RUN_NAME, config={\n",
    "    \"img_size\": IMG_SIZE, \"lr\": 1e-4, \n",
    "    \"batch_size\": BATCH_SIZE, \"accumulation_steps\": ACCUMULATION_STEPS, \n",
    "    \"epochs\": EPOCHS, \"patience\": PATIENCE})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae33d8a",
   "metadata": {},
   "source": [
    "## Data Loader \n",
    "\n",
    "Previously we load the data and then performed preprocessing on the fly\n",
    "Given we did the preprocessing offline, we can now simply load the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b82fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastODIRDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        # Target classes: Normal, Diabetes, Glaucoma, Cataract, AMD, Hypertension, Myopia, Other\n",
    "        self.labels = df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        l_img_path = os.path.join(self.img_dir, row['Left-Fundus'])\n",
    "        r_img_path = os.path.join(self.img_dir, row['Right-Fundus'])\n",
    "        \n",
    "        # Load and Preprocess\n",
    "        l_img = cv2.cvtColor(cv2.imread(l_img_path), cv2.COLOR_BGR2RGB)\n",
    "        r_img = cv2.cvtColor(cv2.imread(r_img_path), cv2.COLOR_BGR2RGB )\n",
    "        \n",
    "        if self.transform:\n",
    "            l_img = self.transform(l_img)\n",
    "            r_img = self.transform(r_img)\n",
    "            \n",
    "        return l_img, r_img, torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13a857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "class ODIRDualNet(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        # Using B0 for efficiency, upgrade to B4 for better accuracy\n",
    "        self.backbone = models.efficientnet_b0(weights='DEFAULT') # use pretrained weights for better feature extraction\n",
    "        # ref https://docs.pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights\n",
    "        self.feature_dim = self.backbone.classifier[1].in_features # Get feature dimension before classifier\n",
    "        self.backbone.classifier = nn.Identity() # Remove top layer\n",
    "        self.features = self.backbone.features # Extract feature extractor part for checkpointing\n",
    "        self.classifier = nn.Sequential( # replace classifier with a custom head that combines features from both eyes\n",
    "            nn.Linear(self.feature_dim * 2, IMG_SIZE), # Combine features from both eyes\n",
    "            nn.ReLU(), # Non-linearity for better learning relu f(x) = max(0, x)\n",
    "            nn.Dropout(0.3), # Regularization to prevent overfitting\n",
    "            nn.Linear(IMG_SIZE, num_classes) # Final output layer for multi-label classification\n",
    "        )\n",
    "\n",
    "    def forward(self, left, right):\n",
    "        # manually checkpoint the feature extraction part to save memory, since EfficientNet can be quite large, especially B4\n",
    "        l_feat = checkpoint(self.features, left, use_reentrant=False)\n",
    "        r_feat = checkpoint(self.features, right, use_reentrant=False)\n",
    "        \n",
    "        # Global Average Pooling to get (Batch, Feat_Dim)\n",
    "        l_feat = torch.flatten(nn.functional.adaptive_avg_pool2d(l_feat, 1), 1)\n",
    "        r_feat = torch.flatten(nn.functional.adaptive_avg_pool2d(r_feat, 1), 1)\n",
    "        combined = torch.cat((l_feat, r_feat), dim=1) # Combine features from both eyes\n",
    "        return self.classifier(combined) # Pass through classifier to get final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b359c",
   "metadata": {},
   "source": [
    "## Thresholds\n",
    "\n",
    "Instead of using 1 thresholds 0.5 for all labels, we find the best or optimise threshold for each label individually to maximise our F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b43d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thresholds(y_true, y_probs):\n",
    "    thresholds = np.linspace(0.1, 0.9, 81) # Test thresholds from 0.1 to 0.9 with fine granularity\n",
    "    best_ts = np.zeros(NUM_CLASSES) \n",
    "    for i in range(NUM_CLASSES):\n",
    "        best_f1 = 0 # Initialize best F1 score for this class\n",
    "        for t in thresholds: # Test each threshold and calculate F1 score \n",
    "            score = f1_score(y_true[:, i], (y_probs[:, i] > t).astype(int), zero_division=0) # zero_division=0 to handle cases where there are no positive predictions\n",
    "            if score > best_f1: \n",
    "                best_f1 = score # Update best F1 score for this class\n",
    "                best_ts[i] = t # Update best threshold for this class\n",
    "    return best_ts # Return array of best thresholds for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4e314",
   "metadata": {},
   "source": [
    "Data Loader with ImageNet Transformation\n",
    "\n",
    "\n",
    "\"All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\"\n",
    "Ref:\n",
    "- https://docs.pytorch.org/vision/0.9/models.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0970a94",
   "metadata": {},
   "source": [
    "model definition and training\n",
    "\n",
    "*We increase POS_WEIGHTS for hypertension and Others*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c29d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CALCULATE POSITIVE WEIGHTS ---\n",
    "def get_pos_weights(df, class_names):\n",
    "    weights = []\n",
    "    for col in class_names:\n",
    "        num_pos = df[col].sum()\n",
    "        num_neg = len(df) - num_pos\n",
    "        # Weight = Count of Negatives / Count of Positives\n",
    "        # We add a small epsilon to avoid division by zero\n",
    "        weight = num_neg / (num_pos + 1e-6)\n",
    "        if col == 'H': \n",
    "            weight = weight * 2.0  # Give extra weight to Hypertension class due to its lower prevalence and importance in this dataset \n",
    "        if col == 'O': \n",
    "            weight = weight * 1.5  # Give extra weight to Other class due to its lower prevalence and importance in this dataset\n",
    "        weights.append(weight)\n",
    "    return torch.tensor(weights, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "pos_weights = get_pos_weights(train_df, CLASS_CODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddbaf9b",
   "metadata": {},
   "source": [
    "Smoothing for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5a3353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Compiled for speed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/600 [00:00<?, ?it/s]W0224 09:27:54.569000 8334 site-packages/torch/_inductor/utils.py:1613] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [04:54<00:00,  2.03it/s] \n",
      "/home/ray/miniconda3/envs/dl/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.1507 | train_f1: 0.2868 | val_f1: 0.4395\n",
      "üöÄ New Best Model Saved! F1: 0.4395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [02:04<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.9756 | train_f1: 0.3644 | val_f1: 0.5385\n",
      "üöÄ New Best Model Saved! F1: 0.5385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:55<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.8696 | train_f1: 0.4259 | val_f1: 0.5855\n",
      "üöÄ New Best Model Saved! F1: 0.5855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.7787 | train_f1: 0.4671 | val_f1: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:51<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.7108 | train_f1: 0.4982 | val_f1: 0.5443\n",
      "üíæ Checkpoint saved at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 0.6476 | train_f1: 0.5298 | val_f1: 0.5777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:51<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 0.5808 | train_f1: 0.5770 | val_f1: 0.5895\n",
      "üöÄ New Best Model Saved! F1: 0.5895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:50<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 0.5381 | train_f1: 0.5919 | val_f1: 0.6116\n",
      "üöÄ New Best Model Saved! F1: 0.6116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:51<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 0.5160 | train_f1: 0.6174 | val_f1: 0.6181\n",
      "üöÄ New Best Model Saved! F1: 0.6181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:52<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 0.4794 | train_f1: 0.6552 | val_f1: 0.6160\n",
      "üíæ Checkpoint saved at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:52<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Loss: 0.4639 | train_f1: 0.6606 | val_f1: 0.6113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:54<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Loss: 0.4556 | train_f1: 0.6624 | val_f1: 0.6160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:52<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Loss: 0.4537 | train_f1: 0.6703 | val_f1: 0.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Loss: 0.4248 | train_f1: 0.6901 | val_f1: 0.6219\n",
      "üöÄ New Best Model Saved! F1: 0.6219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Loss: 0.4267 | train_f1: 0.7004 | val_f1: 0.6353\n",
      "üöÄ New Best Model Saved! F1: 0.6353\n",
      "üíæ Checkpoint saved at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Loss: 0.4075 | train_f1: 0.7107 | val_f1: 0.6415\n",
      "üöÄ New Best Model Saved! F1: 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Loss: 0.4108 | train_f1: 0.7257 | val_f1: 0.6183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Loss: 0.3831 | train_f1: 0.7368 | val_f1: 0.6379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:52<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Loss: 0.3838 | train_f1: 0.7329 | val_f1: 0.6155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Loss: 0.3780 | train_f1: 0.7430 | val_f1: 0.6211\n",
      "üíæ Checkpoint saved at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Loss: 0.3779 | train_f1: 0.7481 | val_f1: 0.6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:52<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Loss: 0.3662 | train_f1: 0.7521 | val_f1: 0.6238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:52<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Loss: 0.3559 | train_f1: 0.7590 | val_f1: 0.6275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:52<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Loss: 0.3608 | train_f1: 0.7577 | val_f1: 0.6248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:52<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Loss: 0.3511 | train_f1: 0.7726 | val_f1: 0.6219\n",
      "üíæ Checkpoint saved at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Loss: 0.3574 | train_f1: 0.7645 | val_f1: 0.6261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:52<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Loss: 0.3487 | train_f1: 0.7685 | val_f1: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Loss: 0.3575 | train_f1: 0.7597 | val_f1: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:54<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Loss: 0.3518 | train_f1: 0.7755 | val_f1: 0.6184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:53<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Loss: 0.3609 | train_f1: 0.7756 | val_f1: 0.6286\n",
      "üíæ Checkpoint saved at epoch 30\n"
     ]
    }
   ],
   "source": [
    "from torch import autocast\n",
    "from torch.amp.grad_scaler import  GradScaler \n",
    "\n",
    "# val and test transforms (no augmentation, just normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# Data augmentation for training set \n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # Rotation and Color Jitter can help the model generalize better by simulating real-world variations in the images\n",
    "        # transforms.RandomRotation(degrees=20),\n",
    "        # transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_loader = DataLoader(FastODIRDataset(train_df, IMG_DIR, train_transform), \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=True)\n",
    "val_loader = DataLoader(FastODIRDataset(val_df, IMG_DIR, transform), \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=NUM_WORKERS, \n",
    "                        pin_memory=True)\n",
    "test_loader = DataLoader(FastODIRDataset(test_df, IMG_DIR, transform), \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=NUM_WORKERS, \n",
    "                         pin_memory=True)\n",
    "model = ODIRDualNet().to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights) # Use pos_weights to handle class imbalance\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2) # reduce learning rate when a metric stop improving\n",
    "scaler = GradScaler(DEVICE)\n",
    "# ‚ö° ACCELERATION: Compile the model (Requires PyTorch 2.0+)\n",
    "# This can provide a 10-20% speedup in training time\n",
    "if hasattr(torch, 'compile'):\n",
    "    model = torch.compile(model)\n",
    "    print(\"‚úÖ Model Compiled for speed.\")\n",
    "\n",
    "start_epoch, best_f1, counter = 0, 0, 0\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    best_ts = checkpoint['thresholds']\n",
    "    counter = checkpoint.get('counter', 0)\n",
    "    start_epoch, best_f1, counter = checkpoint['epoch'] + 1, checkpoint['best_f1'], checkpoint.get('counter', 0)\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        tr_preds, tr_true = [], []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (l, r, y) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            l, r, y = l.to(DEVICE), r.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            with autocast(device_type=DEVICE):\n",
    "                preds = model(l, r) # logits output from the model\n",
    "                loss = criterion(preds, y) / ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            tr_preds.append(torch.sigmoid(preds).detach().cpu().numpy())\n",
    "            tr_true.append(y.cpu().numpy())\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_preds, val_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for l, r, y in val_loader:\n",
    "                out = torch.sigmoid(model(l.to(DEVICE), r.to(DEVICE)))\n",
    "                val_preds.append(out.cpu().numpy())\n",
    "                val_true.append(y.numpy())\n",
    "        torch.cuda.empty_cache() # Explicitly free memory after validation to prevent fragmentation        \n",
    "        val_probs = np.vstack(val_preds)\n",
    "        val_true = np.vstack(val_true)\n",
    "        best_ts = find_best_thresholds(val_true, val_probs)\n",
    "        tr_probs, tr_true = np.vstack(tr_preds), np.vstack(tr_true)\n",
    "        # Calculate Macro F1 with optimized thresholds\n",
    "        val_preds_binary = (val_probs > best_ts).astype(int)\n",
    "        val_f1 = f1_score(val_true, val_preds_binary, average='macro', zero_division=0)\n",
    "        val_acc = accuracy_score(val_true, val_preds_binary)*100. # Convert to percentage for better interpretability\n",
    "        # For training metrics, we can use a fixed threshold of 0.5 \n",
    "        # since we're mainly interested in validation performance for threshold optimization\n",
    "\n",
    "        tr_preds_binary = (tr_probs > 0.5).astype(int)\n",
    "        train_f1 = f1_score(tr_true, tr_preds_binary, average='macro', zero_division=0)\n",
    "        train_acc = accuracy_score(tr_true, tr_preds_binary)*100. # Convert to percentage for better interpretability\n",
    "        # NEW: Step the scheduler based on Validation F1\n",
    "        scheduler.step(val_f1) # pass the metric to scheduler to monitor\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Log per-class F1 for visibility\n",
    "        per_class_f1 = f1_score(val_true, (val_probs > best_ts).astype(int), average=None)\n",
    "        metrics_dict = {f\"val_f1_{name}\": f for name, f in zip(CLASS_NAMES, per_class_f1)}\n",
    "        metrics_dict.update({\"epoch\": epoch+1, \"val_f1\": val_f1, \"train_f1\": train_f1, \"lr\": current_lr, \"train_loss\": train_loss / len(train_loader),\n",
    "                             \"train_acc\": train_acc, \"val_acc\": val_acc})\n",
    "        wandb.log(metrics_dict)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {train_loss/len(train_loader):.4f} | train_f1: {train_f1:.4f} | val_f1: {val_f1:.4f}\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            counter = 0 # reset counter on improvement\n",
    "            torch.save({'model': model.state_dict(), 'thresholds': best_ts}, SAVED_MODEL_PATH)\n",
    "            print(f\"üöÄ New Best Model Saved! F1: {val_f1:.4f}\")\n",
    "        # Save checkpoint every 5 epochs or if no improvement for 3 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'best_f1': best_f1,\n",
    "                'counter': counter,\n",
    "                'thresholds': best_ts\n",
    "            }, CHECKPOINT_PATH)\n",
    "            print(f\"üíæ Checkpoint saved at epoch {epoch+1}\")\n",
    "        if  PATIENCE>0 and counter >= PATIENCE: # early stopping if no improvement for 3 epochs\n",
    "            print(\"‚èπÔ∏è Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00ff47",
   "metadata": {},
   "source": [
    "## Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d98eac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Thresholds and Macro F1 Score of the Best Model:\n",
      "Labels: Normal, Diabetes, Glaucoma, Cataract, AMD, Hypertension, Myopia, Other\n",
      "Best Thresholds: [0.45 0.36 0.88 0.76 0.79 0.81 0.56 0.45]\n",
      "Best Macro F1 Score: 0.6415411400679418\n",
      "‚úÖ Training Complete. Best model and thresholds saved.\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(SAVED_MODEL_PATH, weights_only=False)\n",
    "thresholds = best_model['thresholds']\n",
    "model.load_state_dict(best_model['model'])\n",
    "print(\"Label Thresholds and Macro F1 Score of the Best Model:\")\n",
    "print(\"Labels: Normal, Diabetes, Glaucoma, Cataract, AMD, Hypertension, Myopia, Other\")\n",
    "print(\"Best Thresholds:\", thresholds)\n",
    "print(\"Best Macro F1 Score:\", best_f1)\n",
    "print(\"‚úÖ Training Complete. Best model and thresholds saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2070ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilabel Confusion Matrix:\n",
      "Normal: TP=85, FP=90, TN=121, FN=8\n",
      "Diabetes: TP=82, FP=65, TN=134, FN=23\n",
      "Glaucoma: TP=8, FP=6, TN=282, FN=8\n",
      "Cataract: TP=11, FP=3, TN=285, FN=5\n",
      "AMD: TP=7, FP=15, TN=273, FN=9\n",
      "Hypertension: TP=8, FP=10, TN=284, FN=2\n",
      "Myopia: TP=14, FP=2, TN=286, FN=2\n",
      "Other: TP=73, FP=165, TN=58, FN=8\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.49      0.91      0.63        93\n",
      "    Diabetes       0.56      0.78      0.65       105\n",
      "    Glaucoma       0.57      0.50      0.53        16\n",
      "    Cataract       0.79      0.69      0.73        16\n",
      "         AMD       0.32      0.44      0.37        16\n",
      "Hypertension       0.44      0.80      0.57        10\n",
      "      Myopia       0.88      0.88      0.88        16\n",
      "       Other       0.31      0.90      0.46        81\n",
      "\n",
      "   micro avg       0.45      0.82      0.58       353\n",
      "   macro avg       0.54      0.74      0.60       353\n",
      "weighted avg       0.49      0.82      0.60       353\n",
      " samples avg       0.47      0.82      0.57       353\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_f1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñà‚ñÑ‚ñà‚ñÖ‚ñá‚ñÖ‚ñà‚ñá‚ñÜ‚ñá</td></tr><tr><td>val_f1</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>val_f1_AMD</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá</td></tr><tr><td>val_f1_Cataract</td><td>‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ</td></tr><tr><td>val_f1_Diabetes</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>+5</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_acc</td><td>33.01376</td></tr><tr><td>train_f1</td><td>0.77558</td></tr><tr><td>train_loss</td><td>0.36088</td></tr><tr><td>val_acc</td><td>27.98635</td></tr><tr><td>val_f1</td><td>0.62856</td></tr><tr><td>val_f1_AMD</td><td>0.59259</td></tr><tr><td>val_f1_Cataract</td><td>0.66667</td></tr><tr><td>val_f1_Diabetes</td><td>0.67337</td></tr><tr><td>+5</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-b0_resize_naew</strong> at: <a href='https://wandb.ai/samalo/odir-2019-b/runs/a652j226' target=\"_blank\">https://wandb.ai/samalo/odir-2019-b/runs/a652j226</a><br> View project at: <a href='https://wandb.ai/samalo/odir-2019-b' target=\"_blank\">https://wandb.ai/samalo/odir-2019-b</a><br>Synced 5 W&B file(s), 8 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260224_092735-a652j226/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "t_probs, t_true = [], []\n",
    "with torch.no_grad():\n",
    "    for l, r, y in test_loader:\n",
    "        out = torch.sigmoid(model(l.to(DEVICE), r.to(DEVICE)))\n",
    "        t_probs.append(out.cpu().numpy()); t_true.append(y.numpy())\n",
    "\n",
    "t_p, t_t = np.vstack(t_probs), np.vstack(t_true)\n",
    "t_preds = (t_p > thresholds).astype(int)\n",
    "\n",
    "mcm = multilabel_confusion_matrix(t_t, t_preds)\n",
    "print(\"Multilabel Confusion Matrix:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    tn, fp, fn, tp = mcm[i].ravel()\n",
    "    print(f\"{class_name}: TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n",
    "    wandb.log({f\"CM_{class_name}\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=t_t[:, i],\n",
    "            preds=t_preds[:, i],\n",
    "            title=f\"Confusion Matrix for {class_name}\",\n",
    "            class_names=[\"Absent\", \"Present\"]\n",
    "        )})\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(t_t, t_preds, target_names=CLASS_NAMES))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae021d",
   "metadata": {},
   "source": [
    "## Document Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c76b9fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
       "============================================================================================================================================\n",
       "OptimizedModule                          [1, 3, 512, 512]          [1, 8]                    --                        True\n",
       "‚îú‚îÄODIRDualNet: 1-1                       [1, 3, 512, 512]          [1, 8]                    5,322,884                 True\n",
       "============================================================================================================================================\n",
       "Total params: 5,322,884\n",
       "Trainable params: 5,322,884\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0\n",
       "============================================================================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 6.29\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_data=[\n",
    "    torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE), \n",
    "    torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)], \n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
