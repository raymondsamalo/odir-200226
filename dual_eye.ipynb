{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aac0ba8",
   "metadata": {},
   "source": [
    "# Dual Eyes Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7e6c1",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7fd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q kagglehub torch torchvision scikit-learn pandas opencv-python tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a9960",
   "metadata": {},
   "source": [
    "Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c344cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import kagglehub\n",
    "from tqdm import tqdm # tqdm for progress bars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6af03",
   "metadata": {},
   "source": [
    "Download DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae4a811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/ray/.cache/kagglehub/datasets/andrewmvd/ocular-disease-recognition-odir5k/versions/2\n"
     ]
    }
   ],
   "source": [
    "# 1. Download Dataset (Official ODIR-5K)\n",
    "path = kagglehub.dataset_download(\"andrewmvd/ocular-disease-recognition-odir5k\")\n",
    "print(\"Dataset path:\", path)\n",
    "IMG_DIR = os.path.join(path, \"ODIR-5K/ODIR-5K/Training Images\")\n",
    "CSV_PATH = os.path.join(path, \"full_df.csv\")\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 4\n",
    "ACCUMULATION_STEPS = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 8\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "FAST_IMG_DIR = \"tmp/processed_512_images\"\n",
    "os.makedirs(FAST_IMG_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e383c0",
   "metadata": {},
   "source": [
    "DataSet With Ben Graham's Preprocessing\n",
    "This class implements the Ben Graham Preprocessing on-the-fly and loads both eyes for one patient.\n",
    "ref : \n",
    "- https://scholar.google.com/citations?view_op=view_citation&hl=en&user=jQkkhlkAAAAJ&citation_for_view=jQkkhlkAAAAJ:sNmaIFBj_lkC\n",
    "- https://scholar.google.com/citations?user=jQkkhlkAAAAJ&hl=en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e956c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ben_graham_prep(img, sigmaX=10):\n",
    "    \"\"\"Enhances vessels and normalizes lighting.\"\"\"\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Circular Crop: Find non-black pixels and crop\n",
    "    mask = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) > 10\n",
    "    if np.any(mask):\n",
    "        coords = np.argwhere(mask)\n",
    "        y0, x0 = coords.min(axis=0)\n",
    "        y1, x1 = coords.max(axis=0)\n",
    "        img = img[y0:y1, x0:x1]\n",
    "    \n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    blurred = cv2.GaussianBlur(img, (0, 0), sigmaX)\n",
    "    enhanced = cv2.addWeighted(img, 4, blurred, -4, 128)\n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3bd455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_offline_prep(df, raw_dir, img_prep_func, save_dir):\n",
    "    print(\"ðŸš€ Starting Offline Pre-processing (Ben Graham)...\")\n",
    "    all_images = pd.concat([df['Left-Fundus'], df['Right-Fundus']]).unique()\n",
    "    for img_name in tqdm(all_images):\n",
    "        save_path = os.path.join(save_dir, img_name)\n",
    "        if not os.path.exists(save_path):\n",
    "            img = cv2.imread(os.path.join(raw_dir, img_name))\n",
    "            # Ben Graham Logic\n",
    "            enhanced = img_prep_func(img)\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(enhanced, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd0e0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Offline Pre-processing (Ben Graham)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 754/6716 [00:32<04:25, 22.48it/s]"
     ]
    }
   ],
   "source": [
    "run_offline_prep(df, IMG_DIR, ben_graham_prep, FAST_IMG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastODIRDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        # Target classes: Normal, Diabetes, Glaucoma, Cataract, AMD, Hypertension, Myopia, Other\n",
    "        self.labels = df[['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        l_img_path = os.path.join(self.img_dir, row['Left-Fundus'])\n",
    "        r_img_path = os.path.join(self.img_dir, row['Right-Fundus'])\n",
    "        \n",
    "        # Load and Preprocess\n",
    "        l_img = cv2.cvtColor(cv2.imread(l_img_path), cv2.COLOR_BGR2RGB)\n",
    "        r_img = cv2.cvtColor(cv2.imread(r_img_path), cv2.COLOR_BGR2RGB )\n",
    "        \n",
    "        if self.transform:\n",
    "            l_img = self.transform(l_img)\n",
    "            r_img = self.transform(r_img)\n",
    "            \n",
    "        return l_img, r_img, torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODIRDualNet(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        # Using B0 for efficiency, upgrade to B4 for better accuracy\n",
    "        self.backbone = models.efficientnet_b0(weights='DEFAULT')\n",
    "        self.feature_dim = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity() # Remove top layer\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim * 2, IMG_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(IMG_SIZE, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, left, right):\n",
    "        l_feat = self.backbone(left)\n",
    "        r_feat = self.backbone(right)\n",
    "        combined = torch.cat((l_feat, r_feat), dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b43d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thresholds(y_true, y_probs):\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)\n",
    "    best_ts = np.zeros(NUM_CLASSES)\n",
    "    for i in range(NUM_CLASSES):\n",
    "        best_f1 = 0\n",
    "        for t in thresholds:\n",
    "            score = f1_score(y_true[:, i], (y_probs[:, i] > t).astype(int), zero_division=0)\n",
    "            if score > best_f1:\n",
    "                best_f1 = score\n",
    "                best_ts[i] = t\n",
    "    return best_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4e314",
   "metadata": {},
   "source": [
    "Data Loader with ImageNet Transformation\n",
    "\n",
    "\n",
    "\"All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\"\n",
    "Ref:\n",
    "- https://docs.pytorch.org/vision/0.9/models.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0970a94",
   "metadata": {},
   "source": [
    "model definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autocast\n",
    "from torch.amp.grad_scaler import  GradScaler \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(FastODIRDataset(train_df, FAST_IMG_DIR, transform), \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            shuffle=True,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            pin_memory=True)\n",
    "val_loader = DataLoader(FastODIRDataset(val_df, FAST_IMG_DIR, transform), batch_size=BATCH_SIZE,num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "model = ODIRDualNet().to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler(DEVICE)\n",
    "best_overall_f1 = 0\n",
    "# âš¡ ACCELERATION: Compile the model (Requires PyTorch 2.0+)\n",
    "# This can provide a 10-20% speedup in training time\n",
    "if hasattr(torch, 'compile'):\n",
    "    model = torch.compile(model)\n",
    "    print(\"âœ… Model Compiled for speed.\")\n",
    "for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (l, r, y) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            l, r, y = l.to(DEVICE), r.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            with autocast(device_type=DEVICE):\n",
    "                preds = model(l, r)\n",
    "                loss = criterion(preds, y) / ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * ACCUMULATION_STEPS\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_preds, val_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for l, r, y in val_loader:\n",
    "                out = torch.sigmoid(model(l.to(DEVICE), r.to(DEVICE)))\n",
    "                val_preds.append(out.cpu().numpy())\n",
    "                val_true.append(y.numpy())\n",
    "        \n",
    "        val_probs = np.vstack(val_preds)\n",
    "        val_true = np.vstack(val_true)\n",
    "        best_ts = find_best_thresholds(val_true, val_probs)\n",
    "        \n",
    "        # Calculate Macro F1 with optimized thresholds\n",
    "        f1 = f1_score(val_true, (val_probs > best_ts).astype(int), average='macro')\n",
    "        print(f\"Loss: {train_loss/len(train_loader):.4f} | Val Macro F1: {f1:.4f}\")\n",
    "\n",
    "        if f1 > best_overall_f1:\n",
    "            best_overall_f1 = f1\n",
    "            torch.save({'model': model.state_dict(), 'thresholds': best_ts}, \"best_odir_f1.pth\")\n",
    "            print(\"ðŸš€ New Best Model Saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
